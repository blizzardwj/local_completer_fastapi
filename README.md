# local_completer_fastapi
Input completion anywhere powered by a local LLM run by Ollama or other inference engine.
